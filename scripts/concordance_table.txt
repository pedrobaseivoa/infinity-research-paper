4.5 Concordance Performance

| Concordance Category                        | Claude 3.5 Sonnet     | DeepSeek V3           |
|---------------------------------------------|------------------------|-----------------------|
| Category A (Equivalent)                     | 36 (23.7%)        | 32 (21.1%)       |
| Category B (Concordant with Detail)        | 70 (46.1%)        | 92 (60.5%)       |
| Category C (Concordant with gaps in non-critical) | 21 (13.8%)        | 15 ( 9.9%)        |
| Category D (Factually Divergent)           | 14 ( 9.2%)         |  7 ( 4.6%)        |
| Category E (Conceptually Different)        |  6 ( 3.9%)         |  2 ( 1.3%)        |
| Category F (Incomparable)                  |  5 ( 3.3%)         |  4 ( 2.6%)        |
| **Strong Concordance (A+B)**               | **106 (69.7%)**    | **124 (81.6%)**   |
| **General Concordance (A+B+C)**            | **127 (83.6%)**    | **139 (91.4%)**   |

Automated extraction accuracy was evaluated through a structured comparison between
machine-generated outputs and manually curated gold-standard data across cross
comparisons (152 articles Ã— 8 scientific fields). Each extraction was assessed using a six-level
concordance classification, revealing nuanced patterns of agreement and conflict. Strong
Concordance (Categories A + B) was observed in 106 cases (69.7%) for Claude and 124
cases (81.6%) for DeepSeek, where automated outputs either matched or preserved or
preserved all core content while adding beneficial details. General Concordance (A + B + C),
which also includes core manual information present with gaps in non-critical details